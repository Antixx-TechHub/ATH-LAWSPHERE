# Railway Deployment with Local LLM (Ollama)

This guide explains how to deploy Lawsphere to Railway.app with integrated Ollama support for hosting local LLMs.

## Why Ollama on Railway?

**Cost Savings:**
- OpenAI GPT-4: $0.03 per query = $300+ per 10,000 queries
- Ollama Local: $0 per query = Just Railway compute costs (~$7-15/month)
- **Savings: 90-95% reduction in API costs**

**Performance:**
- Ollama local: 3-5 second latency (no API calls)
- Cloud APIs: 2-3 seconds + network overhead
- First response: 8-12 seconds (model loads once)

**Privacy:**
- All queries stay within your infrastructure
- No data sent to OpenAI, Anthropic, or Google
- GDPR/HIPAA compliant for sensitive data

## Quick Start (5 minutes)

### 1. Fork Repository & Connect to Railway

```bash
# Clone and push your fork to GitHub
git clone https://github.com/your-username/lawsphere.git
cd lawsphere
git remote set-url origin https://github.com/your-username/lawsphere.git
git push -u origin main
```

### 2. Create Railway Project

1. Go to [railway.app](https://railway.app)
2. Click "New Project"
3. Select "Deploy from GitHub repo"
4. Connect your GitHub account
5. Select your lawsphere fork
6. Railway will auto-detect docker-compose and create services

### 3. Set Environment Variables

In Railway Dashboard → Your Project → Variables:

```
# Database & Cache
DATABASE_URL=postgresql://[generated by Railway]
REDIS_PASSWORD=your-secure-password

# Authentication
NEXTAUTH_URL=https://your-app-name.railway.app
NEXTAUTH_SECRET=openssl rand -base64 32

# OAuth Providers (optional)
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=

# ============ LOCAL LLM ============
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2

# Cloud API Keys (optional fallback)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GOOGLE_API_KEY=

# LangSmith (optional)
LANGSMITH_API_KEY=
LANGCHAIN_TRACING_V2=true
```

### 4. Deploy

Railway auto-deploys on git push to main/develop:

```bash
git add .
git commit -m "Deploy to Railway"
git push
```

### 5. Initialize Ollama Model

After first deployment, pull the default model:

```bash
# SSH into Railway container
railway shell

# Pull Qwen 2.5 model (recommended)
./scripts/init-ollama.sh

# Or manually
docker exec lawsphere-ollama-railway ollama pull qwen2
```

**Done!** Your app is now live with free local LLMs.

---

## Docker Compose Files

### `docker-compose.railway.yml` (Main Deployment)

Used for Railway deployments with Ollama integrated:

```yaml
services:
  postgres:      # PostgreSQL with pgvector
  redis:         # Redis cache
  ollama:        # Local LLM hosting (NEW!)
  ai-service:    # FastAPI backend
  web:           # Next.js frontend
```

**Key Features:**
- Ollama service included for local LLM
- All services have health checks
- Persistent volumes for models and data
- Environment variables for configuration

### `docker-compose.prod.yml` (Production Fallback)

Used for production deployments when Ollama not available:
- Optional Ollama (can be disabled)
- Cloud API fallback configuration
- Production-optimized settings

---

## Architecture

```
┌─────────────────────────────────────────────┐
│           Railway.app Platform              │
├─────────────────────────────────────────────┤
│ ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│ │   Web    │  │    AI    │  │ Ollama   │  │
│ │ Next.js  │  │ FastAPI  │  │  LLM     │  │
│ │ Port3000 │  │ Port8000 │  │ Port1143 │  │
│ └──┬───────┘  └──┬───────┘  └──┬───────┘  │
│    │             │              │         │
│ ┌──┴─────────────┴──────────────┴──────┐  │
│ │     PostgreSQL + Redis Cache        │  │
│ │      (Managed by Railway)            │  │
│ └──────────────────────────────────────┘  │
└─────────────────────────────────────────────┘
```

## Model Options

| Model | Size | Speed | Quality | Memory | Use Case |
|-------|------|-------|---------|--------|----------|
| **Qwen 2.5 3B** | 3B | ⚡⚡⚡ | ⭐⭐⭐ | 2GB | Fast testing |
| **Qwen 2.5 7B** | 7B | ⚡⚡ | ⭐⭐⭐⭐ | 4GB | **Recommended** |
| **Qwen 2.5 14B** | 14B | ⚡ | ⭐⭐⭐⭐⭐ | 8GB | Complex tasks |
| **Llama 3.1 8B** | 8B | ⚡⚡ | ⭐⭐⭐⭐ | 4GB | Good alternative |
| **Mistral 7B** | 7B | ⚡⚡ | ⭐⭐⭐⭐ | 4GB | Code/reasoning |

**Recommendation:** Start with **Qwen 2.5 7B** - best balance of speed and quality.

## Model Management

### Pull a Model

```bash
# SSH into Railway container
railway shell

# Pull model
docker exec lawsphere-ollama-railway ollama pull qwen2

# Or use init script
./scripts/init-ollama.sh
```

### List Available Models

```bash
docker exec lawsphere-ollama-railway ollama list
```

### Remove a Model

```bash
docker exec lawsphere-ollama-railway ollama rm qwen2:3b
```

### Change Default Model

Update in Railway variables:
```
OLLAMA_MODEL=llama2:7b
```

Then restart services.

---

## Monitoring

### Check Service Status

```bash
# SSH into Railway
railway shell

# Check all services
docker ps

# View logs
docker logs lawsphere-ai-railway
docker logs lawsphere-ollama-railway
docker logs lawsphere-web-railway
```

### Ollama Health

```bash
docker exec lawsphere-ollama-railway curl http://localhost:11434/api/tags
```

Response shows available models:
```json
{
  "models": [
    {
      "name": "qwen2:latest",
      "size": 4040803184,
      "digest": "...",
      "details": {...}
    }
  ]
}
```

### AI Service Health

```bash
curl https://your-app.railway.app/health
```

### Memory & Disk Usage

```bash
# Check container sizes
docker system df

# Monitor resource usage
docker stats
```

---

## Troubleshooting

### Issue: "Ollama not responding"

**Solution 1:** Check if Ollama is running
```bash
docker ps | grep ollama
docker logs lawsphere-ollama-railway
```

**Solution 2:** Restart Ollama
```bash
docker restart lawsphere-ollama-railway
```

**Solution 3:** Pull model again
```bash
docker exec lawsphere-ollama-railway ollama pull qwen2
```

### Issue: "Out of memory" errors

**Solution:** Use smaller model
```
OLLAMA_MODEL=qwen2:3b  # Instead of qwen2:7b
```

Railway containers typically have 4GB. Use:
- 3B model: 2GB
- 7B model: 4GB (tight fit)
- 14B model: 8GB (need upgrade)

### Issue: Slow first response

**Expected:** First response takes 8-12 seconds (model loading)

**Check:** Model is loaded
```bash
docker exec lawsphere-ollama-railway ollama list
```

Subsequent requests are 3-5 seconds.

### Issue: Can't SSH into Railway container

**Solution:** Install Railway CLI
```bash
npm install -g @railway/cli
railway login
railway shell
```

---

## Cost Breakdown

### Railway Pricing (Monthly)

| Component | Size | Cost |
|-----------|------|------|
| Compute | 2 vCPU | $7-15 |
| Postgres | 10GB | $5-10 |
| Redis | 1GB | $2-5 |
| **Total** | | **$14-30/month** |

### API Comparison

**Using Ollama (Local):**
- 1000 requests/day = **$0/month** (just Railway compute)
- Privacy: ✅ All data local
- Latency: ✅ No external API calls

**Using OpenAI API:**
- 1000 requests/day @ $0.015 per request = **$450/month**
- Privacy: ❌ Data sent to OpenAI
- Latency: ⚠️ External API calls

**Savings with Ollama: 95%+**

---

## Advanced Configuration

### Enable Cloud API Fallback

If Ollama times out, auto-fallback to OpenAI:

```env
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://ollama:11434
OPENAI_API_KEY=sk-your-key
```

Backend automatically tries Ollama first, falls back to OpenAI on timeout.

### GPU Acceleration (Advanced)

If using Railway GPU addon (expensive):

```yaml
environment:
  OLLAMA_NUM_GPU: 1
```

**Not recommended** - CPU is usually sufficient for 7B models.

### Custom Ollama Configuration

Create `ollama.conf` and mount:

```yaml
volumes:
  - ./ollama.conf:/etc/ollama/ollama.conf
```

### Multiple Models

```bash
docker exec lawsphere-ollama-railway ollama pull qwen2
docker exec lawsphere-ollama-railway ollama pull llama2
docker exec lawsphere-ollama-railway ollama pull mistral
```

Then switch via `OLLAMA_MODEL` env var.

---

## Deployment Checklist

- [ ] Repository forked and pushed to GitHub
- [ ] Railway project created and connected to GitHub
- [ ] All environment variables set in Railway
- [ ] Database and Redis services created
- [ ] Web and AI services deployed successfully
- [ ] Ollama service deployed and model pulled
- [ ] Health checks passing for all services
- [ ] Frontend accessible at https://your-app.railway.app
- [ ] Backend responding at /health endpoint
- [ ] Chat working with local LLM model
- [ ] Files tab functional with uploads/downloads

---

## Updating Deployment

### Push Code Changes

```bash
git add .
git commit -m "Feature: Add new feature"
git push origin main
```

Railway auto-deploys on git push.

### Update Environment Variables

1. Go to Railway Dashboard
2. Select project
3. Click "Variables"
4. Update values
5. Services auto-restart

### Update Ollama Model

```bash
railway shell
docker exec lawsphere-ollama-railway ollama pull llama2
```

Then update `OLLAMA_MODEL=llama2` in Railway variables.

---

## Performance Tuning

### Response Time Optimization

1. **First request:** 8-12s (model loads)
2. **Keep warm:** Keep request frequency
3. **Reduce latency:** Use local queries only

### Memory Management

Monitor with:
```bash
docker stats lawsphere-ollama-railway
```

Typical:
- At rest: 50-100MB
- Loaded model: 4-8GB
- Running query: 4-8GB

### Database Optimization

PostgreSQL with pgvector is pre-optimized. Monitor:
```bash
railway shell
psql -c "SELECT pg_size_pretty(pg_database_size('lawsphere'));"
```

---

## Support & Resources

- **Railway Docs:** https://docs.railway.app
- **Ollama Docs:** https://github.com/ollama/ollama
- **FastAPI Docs:** https://fastapi.tiangolo.com
- **Next.js Docs:** https://nextjs.org/docs

---

## Security Notes

### Production Safety

✅ **Local LLM (Recommended):**
- No external API keys needed
- No data leaves your infrastructure
- No rate limiting issues
- GDPR/HIPAA compliant

⚠️ **Cloud API Fallback:**
- Keep API keys secure in Railway variables
- Never commit API keys to git
- Rotate keys regularly
- Monitor usage for abuse

### Database Security

- Change default POSTGRES_PASSWORD
- Use strong REDIS_PASSWORD
- Enable HTTPS (Railway handles automatically)
- Regular backups (Railway managed)

---

## Feedback & Issues

Found a problem? Have a suggestion?

1. Check [LOCAL_LLM_SIMPLIFIED.md](LOCAL_LLM_SIMPLIFIED.md)
2. Check Railway docs for service-specific issues
3. Open an issue on GitHub with detailed steps to reproduce

