# Lawsphere Development Session Notes

> Last Updated: December 28, 2025

---

## ğŸ¯ Project Overview

**Lawsphere** is a legal-tech AI platform built for Indian legal professionals with a **privacy-first architecture**. 

**Target Market**: Indian lawyers, law firms, legal departments
**Key Differentiator**: Sensitive legal data NEVER leaves the local environment

---

## âœ… Completed Features

### 1. Core Application Structure
- **Frontend**: Next.js 14 with App Router, Tailwind CSS, Radix UI
- **Backend**: FastAPI with LangGraph
- **Database**: PostgreSQL with Prisma ORM
- **Cache**: Redis
- **Storage**: MinIO (S3-compatible)

### 2. Authentication System
- NextAuth.js with credentials provider
- Login/Register pages
- Protected dashboard routes

### 3. Dashboard UI
- Sidebar navigation
- Top bar with user menu
- Chat interface
- Files panel
- Notes panel
- Settings page

### 4. Trust-First LLM Routing (Privacy Architecture)

#### Privacy Scanner (`apps/ai-service/app/routing/privacy_scanner.py`)
Detects sensitive content:
- **Indian PII**: Aadhaar (1234-5678-9012), PAN (ABCDE1234F), Phone (+91...)
- **Legal Markers**: Attorney-client privilege, confidential, case numbers
- **Document Types**: Vakalatnama, affidavit, NDA, contracts
- **Financial**: Bank accounts, IFSC codes, settlements

#### Trust Router (`apps/ai-service/app/routing/trust_router.py`)
Routes requests based on sensitivity:
```
SENSITIVE DATA â†’ ğŸ”’ LOCAL MODEL (Ollama)
GENERIC QUERIES â†’ â˜ï¸ CLOUD MODEL (Gemini Flash)
```

#### Audit Logger (`apps/ai-service/app/routing/audit_logger.py`)
- Logs all routing decisions
- Hashes content (never stores plaintext)
- Compliance-ready JSONL format
- Daily log files in `logs/audit/`

#### Trust Chat API (`apps/ai-service/app/api/trust_chat.py`)
New endpoints:
- `POST /api/chat/trust/completions` - Privacy-aware chat
- `GET /api/chat/trust/dashboard` - Privacy metrics
- `GET /api/chat/trust/stats` - Statistics
- `GET /api/chat/trust/models` - Available models
- `GET /api/chat/trust/routing-rules` - Transparency rules

### 5. Ollama Integration (`apps/ai-service/app/models/ollama_client.py`)
- Async HTTP client for local inference
- Health checking
- Model listing
- Chat and generate endpoints
- Streaming support

### 6. UI Components

#### Trust Badge (`apps/web/src/components/chat/trust-badge.tsx`)
- `TrustBadge` - Full trust info display
- `TrustIndicator` - Compact local/cloud indicator
- `TrustDashboard` - Privacy metrics dashboard
- `TrustModelSelector` - Model picker with trust grouping

#### Chat Panel (`apps/web/src/components/chat/chat-panel.tsx`)
- Model selector grouped by Local/Cloud
- Trust indicators on AI messages
- Overflow handling for code blocks

#### Footer (`apps/web/src/components/layout/footer.tsx`)
- ATH Tech Hub branding
- Social links (LinkedIn, GitHub, Website)
- Copyright notice

---

## ğŸ“ Key File Locations

### Backend (AI Service)
```
apps/ai-service/
â”œâ”€â”€ main.py                          # FastAPI app entry
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py                    # Settings & env vars
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ legal_assistant.py       # LangGraph agent
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat.py                  # Basic chat endpoints
â”‚   â”‚   â”œâ”€â”€ trust_chat.py            # Privacy-aware chat â­
â”‚   â”‚   â”œâ”€â”€ files.py                 # File handling
â”‚   â”‚   â”œâ”€â”€ health.py                # Health checks + Ollama status
â”‚   â”‚   â””â”€â”€ search.py                # Legal search
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ llm_router.py            # Model routing
â”‚   â”‚   â””â”€â”€ ollama_client.py         # Ollama integration â­
â”‚   â””â”€â”€ routing/
â”‚       â”œâ”€â”€ privacy_scanner.py       # PII detection â­
â”‚       â”œâ”€â”€ trust_router.py          # Routing logic â­
â”‚       â””â”€â”€ audit_logger.py          # Compliance logging â­
```

### Frontend (Web)
```
apps/web/src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ dashboard/                   # Protected pages
â”‚   â”œâ”€â”€ auth/                        # Login/Register
â”‚   â””â”€â”€ api/                         # API routes
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ chat-panel.tsx           # Main chat UI â­
â”‚   â”‚   â”œâ”€â”€ trust-badge.tsx          # Trust indicators â­
â”‚   â”‚   â”œâ”€â”€ files-panel.tsx
â”‚   â”‚   â””â”€â”€ notes-panel.tsx
â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”œâ”€â”€ sidebar.tsx
â”‚   â”‚   â”œâ”€â”€ top-bar.tsx
â”‚   â”‚   â””â”€â”€ footer.tsx
â”‚   â””â”€â”€ ui/                          # Radix UI components
â””â”€â”€ lib/
    â”œâ”€â”€ auth.ts                      # NextAuth config
    â””â”€â”€ prisma.ts                    # Database client
```

### Documentation
```
docs/
â”œâ”€â”€ LOCAL_LLM_SETUP.md               # Ollama setup guide â­
â””â”€â”€ SESSION_NOTES.md                 # This file
```

---

## ğŸ”§ Configuration

### Environment Variables

**AI Service** (`apps/ai-service/.env`):
```env
GOOGLE_API_KEY=your_gemini_key
OPENAI_API_KEY=your_openai_key        # Optional
ANTHROPIC_API_KEY=your_anthropic_key  # Optional
DEFAULT_MODEL=gemini-2.0-flash-exp
OLLAMA_BASE_URL=http://localhost:11434
```

**Web App** (`apps/web/.env`):
```env
DATABASE_URL=postgresql://...
NEXTAUTH_SECRET=your_secret
NEXTAUTH_URL=http://localhost:3000
AI_SERVICE_URL=http://localhost:8000
```

---

## ğŸš€ How to Run

### Start All Services
```bash
# Option 1: Script
./scripts/start-all.cmd   # Windows
./scripts/start-all.sh    # Linux/Mac

# Option 2: Docker
docker-compose up -d

# Option 3: Manual
# Terminal 1: Database & Redis
docker-compose up postgres redis minio -d

# Terminal 2: AI Service
cd apps/ai-service
python -m uvicorn main:app --port 8000 --reload

# Terminal 3: Web App
cd apps/web
npm run dev
```

### Start Ollama (for local inference)
```bash
ollama serve
ollama pull qwen2.5:7b
ollama pull llama3.1:8b
```

### URLs
| Service | URL |
|---------|-----|
| Web App | http://localhost:3000 |
| AI Service | http://localhost:8000 |
| API Docs | http://localhost:8000/docs |
| Ollama | http://localhost:11434 |

---

## â³ Pending Tasks

### High Priority
1. **Test Local Inference on Powerful Machine**
   - Need 32GB+ RAM or GPU for Ollama
   - Current dev machine has only 16GB RAM, no GPU
   - Follow `docs/LOCAL_LLM_SETUP.md`

2. **Connect Frontend to Trust Chat API**
   - Update `apps/web/src/lib/api/ai-client.ts` to use `/api/chat/trust/completions`
   - Display trust info in chat messages

3. **Add Trust Dashboard Page**
   - Create `/dashboard/privacy` route
   - Show privacy metrics from `/api/chat/trust/dashboard`

### Medium Priority
4. **File Upload with Local Processing**
   - Upload to MinIO
   - Extract text locally
   - Route to local LLM

5. **Legal Document Templates**
   - Contract drafts
   - Affidavits
   - Vakalatnama

6. **Search Integration**
   - Indian legal databases
   - Case law search

### Low Priority
7. **User Management**
   - Roles (Admin, Lawyer, Paralegal)
   - Organization support

8. **Billing Integration**
   - Track token usage
   - Razorpay integration

---

## ğŸ’¡ Key Decisions Made

### Why Privacy-First Routing?
- Indian legal data is highly sensitive
- Client trust is paramount
- Regulatory compliance (upcoming DPDP Act)
- Competitive differentiator for Indian market

### Why Ollama for Local Inference?
- Free and open source
- Simple API (OpenAI compatible)
- Supports latest models (Qwen, Llama, Mistral)
- Easy Docker deployment

### Why Gemini Flash for Cloud?
- Cheapest: $0.000075/1K tokens (â‚¹0.006)
- Fast: ~500ms latency
- Good quality for general queries
- Generous free tier

### Model Selection
| Use Case | Model | Why |
|----------|-------|-----|
| Sensitive docs | Qwen 2.5 7B (Local) | Best multilingual, free |
| Quick local | Llama 3.1 8B (Local) | Fast, good reasoning |
| Cloud queries | Gemini Flash | Cheap, fast |
| Complex analysis | GPT-4o | Best quality ($$) |

---

## ğŸ› Known Issues

1. **Ollama Memory**: 7B models need ~6GB RAM free
   - Solution: Use 3B model or dedicated server

2. **First Model Load Slow**: 30-60 seconds on first request
   - Solution: Pre-warm models on startup

3. **Windows `nul` File**: Git can't track Windows reserved names
   - Solution: Deleted the files

---

## ğŸ“ Contacts

- **Project**: Lawsphere by ATH Tech Hub
- **Website**: https://antixxtechhub.com
- **LinkedIn**: https://www.linkedin.com/company/antixx-tech-hub

---

## ğŸ“ Session History

### December 28, 2025
- Fixed footer layout (logo, powered by text)
- Added colorful social link icons
- Fixed chat panel overflow for code blocks
- Discussed LLM routing strategies (LiteLLM, semantic routing)
- Implemented Trust-First Architecture:
  - Privacy Scanner
  - Trust Router
  - Audit Logger
  - Trust Chat API
  - Trust Badge UI
- Installed Ollama on Windows
- Pulled Qwen 2.5 7B and Llama 3.1 8B models
- Discovered memory limitations (16GB RAM insufficient)
- Created LOCAL_LLM_SETUP.md documentation
- Created this SESSION_NOTES.md

### Previous Sessions
- Initial project setup
- Database schema design
- Authentication implementation
- Dashboard UI creation
- Basic chat functionality

---

*This file is version controlled. Update it as you make progress!*
